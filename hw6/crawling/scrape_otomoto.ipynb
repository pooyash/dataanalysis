{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310fd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import locale\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "'''\n",
    "Webscrapes all otomoto car offers and saves output csv files to /scraped_data folder\n",
    "'''\n",
    "\n",
    "\n",
    "def get_url(brand, model=None):\n",
    "    template = \"https://www.otomoto.pl/osobowe/{}/?search%5Border%5D=created_at_first%3Adesc&search%5Bbrand_program_id%5D%5B0%5D=&search%5Bcountry%5D=\"\n",
    "    if model:\n",
    "        url = template.format(f\"{brand}/{model}\")\n",
    "    else:\n",
    "        url = template.format(f\"{brand}\")\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_car_brands():\n",
    "    \"\"\"\n",
    "    return list of tuples (brand_name, number of brand offers)\n",
    "    \"\"\"\n",
    "    main_http = requests.get(\"https://www.otomoto.pl/osobowe/\")\n",
    "    soup = BeautifulSoup(main_http.text, 'html.parser')\n",
    "\n",
    "    brand_filter = soup.find_all(\"div\", {\"class\": \"filter-item rel\"})[1]\n",
    "    brand_select = brand_filter.find('select')\n",
    "\n",
    "    car_brands = list()\n",
    "    for l in brand_select.contents[2:]:\n",
    "        if isinstance(l, NavigableString):\n",
    "            continue\n",
    "        brand_count = re.findall(r\"[0-9]+\", l.text)[0]\n",
    "        brand_count = int(brand_count)\n",
    "        brand_name = l['value']\n",
    "        car_brands.append((brand_name, brand_count))\n",
    "\n",
    "    # brand warszawa must be search as marka_warszawa\n",
    "    car_brands = [('marka_warszawa', count) if brand == 'warszawa' else (\n",
    "        brand, count) for brand, count in car_brands]\n",
    "\n",
    "    return car_brands\n",
    "\n",
    "\n",
    "def get_brand_models(brand, web_driver):\n",
    "    url = f\"https://www.otomoto.pl/osobowe/{brand}/\"\n",
    "    web_driver.get(url)\n",
    "    soup = BeautifulSoup(web_driver.page_source, 'html.parser')\n",
    "\n",
    "    model_filter = soup.find_all(\"div\", {\"class\": \"filter-item rel\"})[1]\n",
    "    model_select = model_filter.find('select')\n",
    "\n",
    "    car_models = set()\n",
    "    for l in model_select.contents:\n",
    "        if isinstance(l, NavigableString):\n",
    "            continue\n",
    "        car_models.add(l['value'])\n",
    "    return list(filter(None, car_models))\n",
    "\n",
    "\n",
    "def get_webscrap_list():\n",
    "    \"\"\"\n",
    "    Otomoto displays only 16000 offers per selected option,\n",
    "    so we need to split searched settings according to this\n",
    "    return list of (brand,model) according to which page will be scraped\n",
    "    \"\"\"\n",
    "\n",
    "    to_scrap = list()\n",
    "    brands = get_car_brands()\n",
    "\n",
    "    web_driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    threshold = 500 * 32  # otomoto allows to explore ma 500 pages 32 offers each\n",
    "\n",
    "    for brand, count in get_car_brands():\n",
    "        if count <= threshold:\n",
    "            to_scrap.append((brand, None))\n",
    "            continue\n",
    "\n",
    "        # if brand have more than 16000 offers than split it to single models\n",
    "        brand_models = get_brand_models(brand, web_driver)\n",
    "        for model in brand_models:\n",
    "            to_scrap.append((brand, model))\n",
    "\n",
    "    return to_scrap\n",
    "\n",
    "\n",
    "def convert_date(date_string):\n",
    "    \"\"\"\n",
    "    Get date in string and format it to dd/mm/yyyy.\n",
    "    \"\"\"\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US')\n",
    "    month_dict = {'stycznia': 'January',\n",
    "                  'lutego': 'February',\n",
    "                  'marca': 'March',\n",
    "                  'kwietnia': 'April',\n",
    "                  'maja': 'May',\n",
    "                  'czerwca': 'June',\n",
    "                  'lipca': 'July',\n",
    "                  'sierpnia': 'August',\n",
    "                  'września': 'September',\n",
    "                  'października': 'October',\n",
    "                  'listopada': 'November',\n",
    "                  'grudnia': 'December'}\n",
    "\n",
    "    for key in month_dict.keys():\n",
    "        date_string = date_string.replace(key, month_dict[key])\n",
    "    date = datetime.strptime(date_string, \"%H:%M, %d %B %Y\")\n",
    "    return date.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "\n",
    "def get_offer_params(url):\n",
    "    \"\"\"\n",
    "    Scrape variables from single offer based on url\n",
    "    \"\"\"\n",
    "    http = requests.get(url)\n",
    "    soup = BeautifulSoup(http.text, 'html.parser')\n",
    "\n",
    "    params_dict = dict()\n",
    "    try:\n",
    "        params_dict[\"URL\"] = url\n",
    "\n",
    "        # ID\n",
    "        ID_num = soup.find(\n",
    "            \"span\", {\"class\": \"offer-meta__value\", \"id\": \"ad_id\"}).text\n",
    "        params_dict[\"ID\"] = ID_num\n",
    "\n",
    "        # offer add date\n",
    "        add_date = soup.find(\"span\", {\"class\": \"offer-meta__value\"}).text\n",
    "        params_dict['date'] = convert_date(add_date)\n",
    "\n",
    "        # price\n",
    "        price = soup.find(\"span\", {\"class\": \"offer-price__number\"}).text\n",
    "        currency = soup.find(\"span\", {\"class\": \"offer-price__currency\"}).text\n",
    "        price_value = price.replace(currency, '').replace(' ', '')\n",
    "        params_dict[\"Price\"] = int(price_value)\n",
    "        params_dict[\"Currency\"] = currency\n",
    "\n",
    "        # location\n",
    "        location = soup.find(\n",
    "            \"span\", {\"class\": \"seller-box__seller-address__label\"}).text.strip()\n",
    "        params_dict[\"Location\"] = location\n",
    "\n",
    "        # car parameters\n",
    "        params_lists = soup.find_all(\"ul\", {\"class\": \"offer-params__list\"})\n",
    "        for params_list in params_lists:\n",
    "            params_items = params_list.find_all(\n",
    "                \"li\", {\"class\": \"offer-params__item\"})\n",
    "\n",
    "            for l in params_items:\n",
    "                if isinstance(l, NavigableString):\n",
    "                    continue\n",
    "                label = l.find(\"span\").text.strip()\n",
    "                value = l.find(\n",
    "                    \"div\", {\"class\": \"offer-params__value\"}).text.strip()\n",
    "                params_dict[label] = value\n",
    "\n",
    "        # Features\n",
    "        car_features_list = list()\n",
    "        offer_features = soup.find_all(\"ul\", {\"class\": \"offer-features__list\"})\n",
    "        for features in offer_features:\n",
    "            features_items = features.find_all(\n",
    "                \"li\", {\"class\": \"offer-features__item\"})\n",
    "            for l in features_items:\n",
    "                car_features_list.append(l.text.strip())\n",
    "        params_dict[\"Features\"] = car_features_list\n",
    "    except Exception as err:\n",
    "        print(f\"Exception: {url} - {err}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    final_keys = ['ID', 'Price', 'Currency', 'Stan', 'Marka pojazdu', 'Model pojazdu', 'Wersja', 'Generacja', 'Rok produkcji', 'Przebieg', 'Moc',\n",
    "                  'Pojemność skokowa', 'Rodzaj paliwa', 'Emisja CO2', 'Napęd', 'Skrzynia biegów', 'Typ', 'Liczba drzwi', 'Kolor', 'Kraj pochodzenia',\n",
    "                  'Pierwszy właściciel', 'Pierwsza rejestracja', 'date', 'Location', 'Features', 'URL']\n",
    "    final_dict = {key: params_dict[key] if (\n",
    "        key in params_dict) else \"\" for key in final_keys}\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def scrape_model_mp(brand, model=None):\n",
    "    offers_params = list()\n",
    "\n",
    "    url = get_url(brand, model)\n",
    "    while True:\n",
    "        print(url)\n",
    "        http = requests.get(url)\n",
    "        soup = BeautifulSoup(http.text, 'html.parser')\n",
    "\n",
    "        offers = soup.find(\"div\", {\"class\": \"offers list\"}).find_all(\"article\")\n",
    "\n",
    "        for offer in offers:\n",
    "            try:\n",
    "                offer_url = offer['data-href']\n",
    "            except Exception as err:\n",
    "                print(f\"Exception: {err}\")\n",
    "                continue\n",
    "\n",
    "            offer_params = get_offer_params(offer_url)\n",
    "            if offer_params:\n",
    "                offers_params.append(offer_params)\n",
    "\n",
    "        if not soup.find(\"li\", {\"class\": \"next abs\"}):\n",
    "            print(f\"saving {brand}-{model}.csv\")\n",
    "            df = pd.pandas.DataFrame.from_records(offers_params)\n",
    "            df.to_csv(f\"./scraped_data/{brand}-{model}.csv\", index=False)\n",
    "            return\n",
    "        else:\n",
    "            url = soup.find(\"li\", {\"class\": \"next abs\"}).a['href']\n",
    "\n",
    "\n",
    "def scrape_otomoto():\n",
    "    print(\"Collecting webscrap list\")\n",
    "    webscrap_list = get_webscrap_list()\n",
    "\n",
    "    n_cpu = mp.cpu_count()\n",
    "    print(f\"Webscraping using {n_cpu} threads\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_cpu) as executor:\n",
    "        executor.map(lambda p: scrape_model_mp(*p), webscrap_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
